# -*- coding: utf-8 -*-
"""Classificador_Imagens_HP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10-KGg39zyewytkkKhJiT_TPZ0pa1j2c9
"""

#!pip install tensorflow==2.10.0 numpy==1.24.3 scikit-learn==1.3.0 matplotlib==3.7.1 seaborn==0.12.2

import os
import tensorflow as tf

from tensorflow.keras import layers, models

from graphs import (
    plot_history,
    plot_confusion,
    plot_roc,
    plot_pr_curve,
    plot_classification_report,
)

# Use directories relative to this script so it works locally and in
# environments like Google Colab.
DATA_DIR = os.path.join(os.path.dirname(__file__), 'dataset')
IMG_SIZE = (224, 224)
BATCH_SIZE = 16
EPOCHS = 10
VALIDATION_SPLIT = 0.2

def create_cnn_model(input_shape=(224, 224, 3), num_classes=None, class_names=None):
    """Constroi uma CNN simples para classificar as imagens."""

    if num_classes is None:
        if class_names is None:
            raise ValueError("Informe num_classes ou class_names")
        num_classes = len(class_names)

    model = models.Sequential([
        layers.Input(shape=input_shape),
        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

def load_dataset(
    data_dir=DATA_DIR,
    img_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    validation_split=VALIDATION_SPLIT,
):
    """Carrega imagens do diretório e retorna datasets de treino e validação."""

    # 1) Carrega os datasets "puros" (ainda com class_names)
    train_raw = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=validation_split,
        subset="training",
        seed=123,
        image_size=img_size,
        batch_size=batch_size,
    )
    val_raw = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=validation_split,
        subset="validation",
        seed=123,
        image_size=img_size,
        batch_size=batch_size,
    )

    # 2) Extrai class_names (agora só haverá 2 pastas/classes no seu DATA_DIR)
    class_names = train_raw.class_names

    # 3) Aplica cache/prefetch
    autotune = tf.data.AUTOTUNE
    train_ds = train_raw.cache().shuffle(1000).prefetch(buffer_size=autotune)
    val_ds   = val_raw.cache().prefetch(buffer_size=autotune)

    return train_ds, val_ds, class_names

def train(model, train_ds, val_ds, epochs=EPOCHS):
    """Treina ``model`` com o dataset de treino e validação."""

    history = model.fit(train_ds, epochs=epochs, validation_data=val_ds)
    return history



def main():
    """Executa o pipeline de treinamento e gera os gráficos de avaliação."""

    # Prepara os datasets de treino e validação
    train_ds, val_ds, class_names = load_dataset()

    # Cria e treina o modelo
    model = create_cnn_model(num_classes=2)
    history = train(model, train_ds, val_ds)

    # Salva o modelo treinado
    model.save('hp_classifier.h5')
    print('Modelo salvo em hp_classifier.h5')

    # Gera gráficos de desempenho
    plot_history(history)
    plot_confusion(model, val_ds, class_names)
    plot_roc(model, val_ds)
    plot_pr_curve(model, val_ds)
    plot_classification_report(model, val_ds, class_names)


if __name__ == '__main__':
    main()

